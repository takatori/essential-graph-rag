{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ch05_tools\n",
    "from ollama import Message\n",
    "\n",
    "tool_picker_prompt = \"\"\"\n",
    "    Your job is to chose the right tool needed to respond to the user question.\n",
    "    The available tools are provided to you in the prompt.\n",
    "    Make sure to pass the right and the complete arguments to the chosen tool.\n",
    "\"\"\"\n",
    "\n",
    "tools = {\n",
    "    \"movie_info_by_title\": {\n",
    "        \"description\": ch05_tools.movie_info_by_title_description,\n",
    "        \"function\": ch05_tools.movie_info_by_title,\n",
    "    },\n",
    "    \"movies_info_by_actor\": {\n",
    "        \"description\": ch05_tools.movies_info_by_actor_description,\n",
    "        \"function\": ch05_tools.movies_info_by_actor,\n",
    "    },\n",
    "    \"text2cypher\": {\n",
    "        \"description\": ch05_tools.text2cypher_description,\n",
    "        \"function\": ch05_tools.text2cypher,\n",
    "    },\n",
    "    \"answer_given\": {\n",
    "        \"description\": ch05_tools.answer_given_description,\n",
    "        \"function\": ch05_tools.answer_given,\n",
    "    }\n",
    "}\n",
    "\n",
    "def handle_tool_calls(tools: dict[str, any], llm_tool_calls: list[Message.ToolCall]):\n",
    "    output = []\n",
    "    if llm_tool_calls:\n",
    "        for tool_call in llm_tool_calls:\n",
    "            function_to_call = tools[tool_call.function.name][\"function\"]\n",
    "            function_args = tool_call.function.arguments\n",
    "            res = function_to_call(**function_args)\n",
    "            output.append(res)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import chat\n",
    "\n",
    "query_update_prompt = \"\"\"\n",
    "    You are an exprert at updating questions to make the them ask for one thing only, more atomic, specific and easier to find the answer for.\n",
    "    You do this by filling in missing information in the question, with the extra information provided to you in previous answers.\n",
    "\n",
    "    You respond with the updated question that has all information in it.\n",
    "    Only edit the question if needed. If the original question already is atomic, specific and easy to answer, you keep the original.\n",
    "    Do not ask for more information than the original question. Only rephrase the question to make it more complete.\n",
    "\n",
    "    JSON template to use:\n",
    "    {\n",
    "        \"question\": \"question1\"\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "def query_update(input: str, answers: list[any]) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": query_update_prompt},\n",
    "        *answers,\n",
    "        {\"role\": \"user\", \"content\": f\"The user question to rewrite: '{input}'\"}\n",
    "    ]\n",
    "    config = {\"format\": \"json\"}\n",
    "    output = chat(    \n",
    "        messages=messages,\n",
    "        config=config\n",
    "    )\n",
    "    try:\n",
    "        return json.loads(output)[\"question\"]\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tool_choice\n",
    "\n",
    "def route_question(question: str, tools: dict[str, any], answers: list[dict[str, str]]): \n",
    "    llm_tool_calls = tool_choice(\n",
    "        [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\": tool_picker_prompt\n",
    "            },\n",
    "            *answers,\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": f\"The user question to find a tool to answer: '{question}'\"\n",
    "            }\n",
    "        ],\n",
    "        tools=[tool[\"description\"] for tool in tools.values()],\n",
    "    )\n",
    "    print(f\"LLM tool calls: {llm_tool_calls}\")\n",
    "    return handle_tool_calls(tools, llm_tool_calls)\n",
    "\n",
    "def handle_user_input(input: str, answers: list[dict[str, str]] = []):\n",
    "    updated_question = query_update(input, answers)\n",
    "    response = route_question(updated_question, tools, answers)\n",
    "    print(response)\n",
    "    answers.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"For the question: '{updated_question}', we have the answer: '{json.dumps(response)}'\"\n",
    "        })\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_critique_prompt = \"\"\"\n",
    "    You are an expert at identifying if questions have been fully answered or if there is an opportunity to enrich the answer.\n",
    "    The user will provide a question, and you will scan through the provided information to see if the question is answered.\n",
    "    If anything is missing from the answer, you will provide a set of new questions that can be asked to gather the missing information.\n",
    "    All new questions must be complete, atomic and specific.\n",
    "    However, if the provided information is enough to answer the original question, you will respond with an empty list.\n",
    "\n",
    "    JSON template to use for finding missing information:\n",
    "    {\n",
    "        \"questions\": [\"question1\", \"question2\"]\n",
    "    }\n",
    "\n",
    "    if the original question is fully answered, you will respond with an empty list.\n",
    "    {\n",
    "        \"questions\": []\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "def critique_answers(question: str, answers: list[dict[str, str]]) -> list[str]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": answer_critique_prompt},\n",
    "        *answers,\n",
    "        {\"role\": \"user\", \"content\": f\"The original user question to answer: '{question}'\"},\n",
    "        \n",
    "    ]\n",
    "    config = {\"format\": \"json\"}\n",
    "    print(f\"Messages for critique: {messages}\")\n",
    "    output = chat(\n",
    "        messages=messages,\n",
    "        config=config\n",
    "    )\n",
    "    try:\n",
    "        print(output)\n",
    "        return json.loads(output)[\"questions\"]\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = \"\"\"\n",
    "    Your job is to help the user with their questions.\n",
    "    You will receive user questions and information neeeded to answer the questions.\n",
    "    If the information is missing to answer part of or the whole question, you will say that the information\n",
    "    is missing. You will only use the information provided to you in the prompt to answer the questions.\n",
    "    You are not allowed to make anything up or use external information.\n",
    "\"\"\"\n",
    "\n",
    "def main(input: str):\n",
    "    answers = handle_user_input(input)\n",
    "    critique = critique_answers(input, answers)\n",
    "\n",
    "    if critique:\n",
    "        answers = handle_user_input(\"\".join(critique), answers)\n",
    "\n",
    "    llm_response = chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": main_prompt},\n",
    "            *answers,\n",
    "            {\"role\": \"user\", \"content\": f\"The user question to answer: '{input}'\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return llm_response, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, answers = main(\"Who's the main actor in the movie Matrix and what other movies is that person in?\")\n",
    "print(f\"Answers: {'\\n'.join([json.dumps(answer) for answer in answers])}\")\n",
    "print(f\"Main response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "next_res, answers = main(\"How many directors and producers are in the database?\")\n",
    "print(f\"Next response: {next_res}\")\n",
    "print(f\"Answers:\\n {'\\n'.join([json.dumps(answer) for answer in answers])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "essential-graph-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
