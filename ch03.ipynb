{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepback_system_message = \"\"\"\n",
    "You are an expert at world knowledge. Your task is to step back\n",
    "and paraphrase a question to a more generic step-back question, which\n",
    "is easier to answer. Here are a few examples\n",
    "\n",
    "\"input\": \"Could the members of The Police perform lawful arrests?\"\n",
    "\"output\": \"what can the members of The Police do?\"\n",
    "\n",
    "\"input\": \"Jan Sindel’s was born in what country?\"\n",
    "\"output\": \"what is Jan Sindel’s personal history?\"\n",
    "\n",
    "Here's a paraphrased version of the question:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_stepback(question: str):\n",
    "    user_message = f\"\"\"{question}\"\"\"\n",
    "    step_back_question = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": stepback_system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "    )\n",
    "    return step_back_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which team did Thierry Audel play for from 2007 to 2008?\"\n",
    "step_back_question = generate_stepback(question)\n",
    "print(f\"Stepback results: {step_back_question['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "remote_pdf_url = \"https://arxiv.org/pdf/1709.00666.pdf\"\n",
    "pdf_filename = \"ch03-downloaded.pdf\"\n",
    "\n",
    "response = requests.get(remote_pdf_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(pdf_filename, \"wb\") as pdf_file:\n",
    "        pdf_file.write(response.content)\n",
    "else:\n",
    "    print(\"Failed to download the PDF. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "with pdfplumber.open(pdf_filename) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text_by_titles(text):\n",
    "    title_pattern = re.compile(r\"(\\n\\d+[A-Z]?\\. {1,3}.{0,60}\\n)\", re.DOTALL)\n",
    "    titles = title_pattern.findall(text)\n",
    "    sections = re.split(title_pattern, text)\n",
    "    sections_with_titles = []\n",
    "    sections_with_titles.append(sections[0])\n",
    "    for i in range(1, len(titles) + 1):\n",
    "        section_text = sections[i * 2 -1].strip() + \"\\n\" + sections[i*2].strip()\n",
    "        sections_with_titles.append(section_text)\n",
    "    return sections_with_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_text_by_titles(text)\n",
    "print(f\"Number of sections: {len(sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    return len(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import chunk_text\n",
    "\n",
    "parent_chunks = []\n",
    "for s in sections:\n",
    "    parent_chunks.extend(chunk_text(s, 200, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parent_chunks[:3])  # Display the first 3 chunks for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import embed\n",
    "from utils import neo4j_driver\n",
    "\n",
    "cypher_import_query = \"\"\"\n",
    "MERGE (pdf:PDF {id:$pdf_id})\n",
    "MERGE (p:Parent {id:$pdf_id + '-' + $id})\n",
    "SET p.text = $parent\n",
    "MERGE (pdf)-[:HAS_PARENT]->(p)\n",
    "WITH p, $children AS children, $embeddings as embeddings\n",
    "UNWIND range(0, size(children) - 1) AS child_index\n",
    "MERGE (c:Child {id: $pdf_id + '-' + $id + '-' + toString(child_index)})\n",
    "SET c.text = children[child_index], c.embedding = embeddings[child_index]\n",
    "MERGE (p)-[:HAS_CHILD]->(c);\n",
    "\"\"\"\n",
    "\n",
    "for i, chunk in enumerate(parent_chunks):\n",
    "    child_chunks = chunk_text(chunk, 500, 20)\n",
    "    embeddings = embed(child_chunks)\n",
    "\n",
    "    neo4j_driver.execute_query(\n",
    "        cypher_import_query,\n",
    "        id=str(i),\n",
    "        pdf_id='1709.00666',\n",
    "        parent=chunk,\n",
    "        children=child_chunks,\n",
    "        embeddings=embeddings.tolist()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_driver.execute_query(\"\"\"\n",
    "MATCH p=(pdf:PDF)-[:HAS_PARENT]->()-[:HAS_CHILD]->()\n",
    "RETURN p LIMIT 25\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_driver.execute_query(\"\"\"CREATE VECTOR INDEX parent IF NOT EXISTS FOR (c:Child) ON c.embedding\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "retrieval_query = \"\"\"\n",
    "CALL db.index.vector.queryNodes($index_name, $k * 4, $question_embedding)\n",
    "YIELD node, score\n",
    "MATCH (node)<-[:HAS_CHILD]-(parent)\n",
    "WITH parent, max(score) AS score\n",
    "RETURN parent.text AS text, score\n",
    "ORDER BY score DESC\n",
    "LIMIT toInteger($k)\n",
    "\"\"\"\n",
    "\n",
    "def parent_retrieval(question: str, k: int = 4) -> List[str]:\n",
    "    question_embedding = embed([question])[0]\n",
    "    similar_records, _, _ = neo4j_driver.execute_query(\n",
    "        retrieval_query,\n",
    "        index_name=\"parent\",\n",
    "        k=k,\n",
    "        question_embedding=question_embedding.tolist()\n",
    "    )\n",
    "    return [r['text'] for r in similar_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You're en Einstein expert, but can only use the provided documents to respond to the question.\"\n",
    "\n",
    "def generate_answer(question: str, documents: List[str]) -> str:\n",
    "    user_message = f\"\"\"\n",
    "Use the following documents to answer the question that will follow:\n",
    "{documents}\n",
    "\n",
    "---\n",
    "\n",
    "The question to answer using information only from the above documents: {question}\n",
    "\"\"\"\n",
    "    \n",
    "    result = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "    )\n",
    "    return result['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(question: str) -> str:\n",
    "    stepback_prompt = generate_stepback(question)\n",
    "    stepback_prompt = stepback_prompt['message']['content']\n",
    "    print(f\"Stepback prompt: {stepback_prompt}\")\n",
    "    documents = parent_retrieval(stepback_prompt)\n",
    "    answer = generate_answer(question, documents)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline(\"Who was the Einsten's collaborator on sound reproduction system?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline(\"When was Einstein granted the patent for his blouse design?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "essential-graph-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
